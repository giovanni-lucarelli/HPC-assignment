\section{Introduction}
\begin{frame}{Goals}
    \begin{enumerate}
        \item \textbf{Parallelize} using hybrid (MPI, OpenMP) approach the stencil method for the 2d heat equation
        \item Perform \textbf{scalability} study:
        \begin{enumerate}
            \item Thread scaling
            \item Strong scaling
            \item Weak scaling
        \end{enumerate}
    \end{enumerate}
\end{frame}

\begin{frame}{Algorithm}
    \textbf{Heat equation (2d)}

    \begin{equation*}
        \partial_t u = \alpha (\partial_x^2 u + \partial_y^2 u)
    \end{equation*}

    \textbf{Finite difference integration} (5-point stencil method)
    \begin{equation*}
        u^{(t+1)}_{i,j} = (1 -4\alpha)u^{(t)}_{i,j} + \alpha \sum_{\langle i,j\rangle} u^{(t)}_{i,j}
    \end{equation*}
    \centering
    $x\in[0,L_x]\rightarrow i \in \{1,\dots, N_x-1\}$\\
    $y\in[0,L_y]\rightarrow j \in \{1,\dots, N_y-1\}$
    
\end{frame}

\begin{frame}{Code Correctness}
    \begin{figure}
    \centering{
        \resizebox{\linewidth}{!}{\input{images/evolution.pgf}}
    }
\end{figure}
\end{frame}

\begin{frame}[fragile]{Build Configuration}
    % \begin{itemize}
    %     \item Compiler flags:\\ \texttt{-O3 -Wall -march=native}
    % \end{itemize}
                \lstset{basicstyle=\ttfamily\small, language=bash}
            \begin{lstlisting}
    CC      = mpicc
    CFLAGS  = -O3 -Wall -fopenmp -march=native
    TARGET  = stencil_mpi

    ENABLE_OUTPUT ?= 0
            \end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Parallelization: shared memory}

    Implementation
    \lstset{basicstyle=\ttfamily\scriptsize, language=C++}

    \begin{lstlisting}
    #pragma omp parallel for schedule(static)
    for (uint j = 1; j <= ysize; j++){
        for ( uint i = 1; i <= xsize; i++){

            // update rule

        }
    }
    \end{lstlisting}
    \vfill
    Thread placement
    \begin{lstlisting}
    export OMP_PLACES=cores
    export OMP_PROC_BIND=close
    \end{lstlisting}

\end{frame}

\begin{frame}[fragile]{First-touch policy}
    \lstset{basicstyle=\ttfamily\scriptsize, language=C++}
    \begin{lstlisting}
int memory_allocate ( ... ){

    planes_ptr[OLD].data = (double*) malloc(frame_elems * sizeof(double));
    planes_ptr[NEW].data = (double*) malloc(frame_elems * sizeof(double));

    #pragma omp parallel for schedule(static)
    for (int j = 0; j < Ny + 2; ++j){
        for (int i = 0; i < Nx + 2; ++i) {
            size_t idx = (size_t)j * (Nx + 2) + i;
            planes_ptr[OLD].data[idx] = 0.0;
            planes_ptr[NEW].data[idx] = 0.0;
        }
    }
}
    \end{lstlisting}
    
\end{frame}

\begin{frame}{Parallelization: distributed memory}
    \begin{figure}
        \includegraphics[width=0.8\textwidth]{./images/send-recv.drawio (1).pdf}
    \end{figure}
\end{frame}

\begin{frame}{Parallelization: distributed memory}
    \begin{figure}
        \includegraphics[width=\textwidth]{./images/send-recv.drawio (2).pdf}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Parallelization: distributed memory}

    For each task:\vfill
    \lstset{basicstyle=\ttfamily\scriptsize, language=C++}

    \begin{lstlisting}
    // pack buffers

    MPI_Irecv(...);
	
    MPI_Isend(...);

    update_internal();

    MPI_Waitall();

    // unpack buffers

    update_border();
    \end{lstlisting}

\end{frame}

% \begin{frame}[fragile]{Assessment: time measures}

%     \begin{itemize}
%         \item Computation time = Internal + Boundary
%         \item Communication time = Packing + Exchange + Unpacking
%     \end{itemize}

% \end{frame}


